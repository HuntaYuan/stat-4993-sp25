{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db278ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import wrds\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcda6b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to WRDS\n",
    "db = wrds.Connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f2d8761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory structure in /Users/dyuan868/Desktop/gpas/4993/stat-4993-sp25/Data\n"
     ]
    }
   ],
   "source": [
    "# Setup directory structure\n",
    "base_dir = \"/Users/dyuan868/Desktop/gpas/4993/stat-4993-sp25/Data\"\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Create main directories\n",
    "dirs = [\n",
    "    \"1_raw_data\", \n",
    "    \"2_processed_data\", \n",
    "    \"3_financial_ratios\", \n",
    "    \"4_industry_datasets\",\n",
    "    \"5_analysis\"\n",
    "]\n",
    "for dir_name in dirs:\n",
    "    os.makedirs(f\"{base_dir}/{dir_name}\", exist_ok=True)\n",
    "print(f\"Created directory structure in {base_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e06fd2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching CRSP monthly stock data...\n",
      "Raw CRSP data saved to /Users/dyuan868/Desktop/gpas/4993/stat-4993-sp25/Data/1_raw_data/crsp_data_2010_2021.csv\n"
     ]
    }
   ],
   "source": [
    "# 1. Fetch CRSP monthly stock data with ticker and industry information\n",
    "print(\"Fetching CRSP monthly stock data...\")\n",
    "crsp_query = \"\"\"\n",
    "    SELECT a.permno,\n",
    "           a.date,\n",
    "           b.ticker,\n",
    "           b.comnam AS company_name,\n",
    "           b.siccd AS sic_code,      -- Standard Industrial Classification code\n",
    "           a.ret AS monthly_return,\n",
    "           a.prc AS price,\n",
    "           ABS(a.prc) AS adj_price,\n",
    "           a.vol AS volume,\n",
    "           a.shrout AS shares_outstanding,  -- Add shares outstanding for market cap calculation\n",
    "           b.shrcd AS share_code,\n",
    "           b.exchcd AS exchange_code,\n",
    "           CASE \n",
    "               WHEN b.exchcd = 1 THEN 'NYSE'\n",
    "               WHEN b.exchcd = 2 THEN 'AMEX'\n",
    "               WHEN b.exchcd = 3 THEN 'NASDAQ'\n",
    "               ELSE 'Other'\n",
    "           END AS exchange_name\n",
    "    FROM crsp.msf AS a\n",
    "    LEFT JOIN crsp.msenames AS b\n",
    "        ON a.permno = b.permno\n",
    "        AND b.namedt <= a.date \n",
    "        AND a.date <= b.nameendt\n",
    "    WHERE a.date BETWEEN '2010-01-01' AND '2021-12-31'  -- Extending to more recent data\n",
    "      AND b.shrcd IN (10, 11)\n",
    "      AND b.exchcd IN (1, 2, 3)\n",
    "\"\"\"\n",
    "crsp_data = db.raw_sql(crsp_query)\n",
    "crsp_data['date'] = pd.to_datetime(crsp_data['date'])\n",
    "crsp_data['year'] = crsp_data['date'].dt.year\n",
    "crsp_data['quarter'] = crsp_data['date'].dt.quarter\n",
    "crsp_data['month'] = crsp_data['date'].dt.month\n",
    "crsp_data['yearmonth'] = crsp_data['year'] * 100 + crsp_data['month']\n",
    "\n",
    "# Calculate market capitalization\n",
    "crsp_data['market_cap'] = crsp_data['adj_price'] * crsp_data['shares_outstanding'] / 1000  # in thousands\n",
    "\n",
    "# Save raw CRSP data\n",
    "crsp_data.to_csv(f\"{base_dir}/1_raw_data/crsp_data_2010_2021.csv\", index=False)\n",
    "print(f\"Raw CRSP data saved to {base_dir}/1_raw_data/crsp_data_2010_2021.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "947e77c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRSP data loaded: (538814, 19)\n",
      "CRSP data with industry classification saved to /Users/dyuan868/Desktop/gpas/4993/stat-4993-sp25/Data/2_processed_data/crsp_with_industry.csv\n"
     ]
    }
   ],
   "source": [
    "# 2. Add industry classification using SIC codes\n",
    "def get_industry_classification(sic_code):\n",
    "    if pd.isna(sic_code):\n",
    "        return 'Unknown'\n",
    "    \n",
    "    sic_str = str(int(sic_code)).zfill(4) if not pd.isna(sic_code) else '0000'\n",
    "    \n",
    "    # Define industry classifications based on SIC codes\n",
    "    if sic_str.startswith(('01', '02', '07', '08', '09')):\n",
    "        return 'Agriculture'\n",
    "    elif sic_str.startswith(('10', '12', '13', '14')):\n",
    "        return 'Mining'\n",
    "    elif sic_str.startswith(('15', '16', '17')):\n",
    "        return 'Construction'\n",
    "    elif sic_str.startswith(('20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39')):\n",
    "        return 'Manufacturing'\n",
    "    elif sic_str.startswith(('40', '41', '42', '43', '44', '45', '46', '47', '48', '49')):\n",
    "        return 'Transportation & Utilities'\n",
    "    elif sic_str.startswith(('50', '51')):\n",
    "        return 'Wholesale Trade'\n",
    "    elif sic_str.startswith(('52', '53', '54', '55', '56', '57', '58', '59')):\n",
    "        return 'Retail Trade'\n",
    "    elif sic_str.startswith(('60', '61', '62', '63', '64', '65', '66', '67')):\n",
    "        return 'Finance'\n",
    "    elif sic_str.startswith(('70', '71', '72', '73', '74', '75', '76', '77', '78', '79')):\n",
    "        return 'Services'\n",
    "    elif sic_str.startswith(('80', '81', '82', '83', '84', '85', '86', '87', '88', '89')):\n",
    "        return 'Health & Education'\n",
    "    elif sic_str.startswith(('91', '92', '93', '94', '95', '96', '97', '98', '99')):\n",
    "        return 'Public Administration'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "# Add industry classification\n",
    "crsp_data['industry'] = crsp_data['sic_code'].apply(get_industry_classification)\n",
    "print(f\"CRSP data loaded: {crsp_data.shape}\")\n",
    "\n",
    "# Save CRSP data with industry classification\n",
    "crsp_data.to_csv(f\"{base_dir}/2_processed_data/crsp_with_industry.csv\", index=False)\n",
    "print(f\"CRSP data with industry classification saved to {base_dir}/2_processed_data/crsp_with_industry.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d354a1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching Compustat quarterly financial data...\n",
      "Raw Compustat data saved to /Users/dyuan868/Desktop/gpas/4993/stat-4993-sp25/Data/1_raw_data/compustat_data_2009_2021.csv\n"
     ]
    }
   ],
   "source": [
    "# 3. Get Compustat quarterly financial data with expanded fields for advanced ratios\n",
    "print(\"Fetching Compustat quarterly financial data...\")\n",
    "comp_query = \"\"\"\n",
    "    SELECT gvkey,\n",
    "           datadate,\n",
    "           fyearq AS fiscal_year,\n",
    "           fqtr AS fiscal_quarter,\n",
    "           rdq AS report_date,\n",
    "           \n",
    "           -- Income Statement Items\n",
    "           revtq AS revenue,\n",
    "           cogsq AS cost_of_goods_sold,\n",
    "           xsgaq AS sga_expense,\n",
    "           oiadpq AS operating_income,\n",
    "           oibdpq AS operating_income_before_depreciation_amortization, -- EBITDA\n",
    "           dpq AS depreciation_amortization,               -- Added for EBITDA calculation\n",
    "           oancfy AS operating_cash_flow,\n",
    "           txtq AS income_tax,\n",
    "           niq AS net_income,\n",
    "           piq AS pretax_income,\n",
    "           ibq AS income_before_extraordinary_items,\n",
    "           xintq AS interest_expense,\n",
    "           dvpq AS dividends_preferred,                   -- Added for dividends\n",
    "           dvpsxq AS dividends_per_share,                 -- Added for dividends yield\n",
    "           \n",
    "           -- Balance Sheet Items\n",
    "           atq AS total_assets,\n",
    "           ltq AS total_liabilities,\n",
    "           ceqq AS common_equity,\n",
    "           seqq AS stockholders_equity,\n",
    "           cheq AS cash_and_equivalents,\n",
    "           rectq AS accounts_receivable,\n",
    "           invtq AS inventories, \n",
    "           ppentq AS ppe_net,\n",
    "           actq AS current_assets,\n",
    "           lctq AS current_liabilities,\n",
    "           dlttq AS long_term_debt,\n",
    "           dlcq AS debt_in_current_liabilities,\n",
    "           apq AS accounts_payable,\n",
    "           txpq AS taxes_payable,                        -- Added for taxes payable\n",
    "           acoq AS other_current_assets,                 -- Added for other current assets\n",
    "           loq AS other_liabilities,                     -- Added for other liabilities\n",
    "           pstkq AS preferred_stock,\n",
    "           cshoq AS common_shares_outstanding,\n",
    "           \n",
    "           -- Other Items\n",
    "           capxy AS capital_expenditure,\n",
    "           tic AS ticker_compustat\n",
    "           \n",
    "    FROM comp.fundq\n",
    "    WHERE datadate BETWEEN '2009-01-01' AND '2021-12-31'  -- Start from 2009 to calculate YoY for 2010\n",
    "      AND indfmt='INDL'\n",
    "      AND datafmt='STD'\n",
    "      AND popsrc='D'\n",
    "      AND consol='C'\n",
    "\"\"\"\n",
    "comp_data = db.raw_sql(comp_query)\n",
    "comp_data['datadate'] = pd.to_datetime(comp_data['datadate'])\n",
    "comp_data['year'] = comp_data['datadate'].dt.year\n",
    "comp_data['quarter'] = comp_data['datadate'].dt.quarter\n",
    "comp_data['yearquarter'] = comp_data['year'] * 10 + comp_data['quarter']\n",
    "\n",
    "# Save raw Compustat data\n",
    "comp_data.to_csv(f\"{base_dir}/1_raw_data/compustat_data_2009_2021.csv\", index=False)\n",
    "print(f\"Raw Compustat data saved to {base_dir}/1_raw_data/compustat_data_2009_2021.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94778bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching CRSP-Compustat link table...\n",
      "CCM link table saved to /Users/dyuan868/Desktop/gpas/4993/stat-4993-sp25/Data/1_raw_data/ccm_link_table.csv\n"
     ]
    }
   ],
   "source": [
    "# 4. Link CRSP and Compustat data using CCM (CRSP/Compustat Merged Database)\n",
    "print(\"Fetching CRSP-Compustat link table...\")\n",
    "ccm_query = \"\"\"\n",
    "    SELECT gvkey, lpermno AS permno, \n",
    "           linkdt, linkenddt\n",
    "    FROM crsp.ccmxpf_linktable\n",
    "    WHERE linktype IN ('LU', 'LC')\n",
    "      AND linkprim IN ('P', 'C')\n",
    "\"\"\"\n",
    "ccm_data = db.raw_sql(ccm_query)\n",
    "ccm_data['linkdt'] = pd.to_datetime(ccm_data['linkdt'])\n",
    "ccm_data['linkenddt'] = pd.to_datetime(ccm_data['linkenddt'])\n",
    "\n",
    "# Fill NaT linkenddt with future date\n",
    "max_date = pd.to_datetime('2025-12-31')\n",
    "ccm_data['linkenddt'] = ccm_data['linkenddt'].fillna(max_date)\n",
    "\n",
    "# Save CCM link table\n",
    "ccm_data.to_csv(f\"{base_dir}/1_raw_data/ccm_link_table.csv\", index=False)\n",
    "print(f\"CCM link table saved to {base_dir}/1_raw_data/ccm_link_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49be811b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading CRSP, Compustat and CCM link table...\n",
      "Processing and merging data (optimized method)...\n",
      "Step 1: Joining CRSP with CCM link table...\n",
      "Step 3: Performing optimized merge...\n",
      "Processing 7413 unique companies...\n",
      "Progress: 9% (741/7413 companies processed)\n",
      "Progress: 19% (1482/7413 companies processed)\n",
      "Progress: 29% (2223/7413 companies processed)\n",
      "Progress: 39% (2964/7413 companies processed)\n",
      "Progress: 49% (3705/7413 companies processed)\n",
      "Progress: 59% (4446/7413 companies processed)\n",
      "Progress: 69% (5187/7413 companies processed)\n",
      "Progress: 79% (5928/7413 companies processed)\n",
      "Progress: 89% (6669/7413 companies processed)\n",
      "Progress: 99% (7410/7413 companies processed)\n",
      "Progress: 100% (7413/7413 companies processed)\n",
      "Merge completed successfully, 528583 records found.\n",
      "Merged CRSP-Compustat data saved to /Users/dyuan868/Desktop/gpas/4993/stat-4993-sp25/Data/2_processed_data/crsp_compustat_merged_20250314.csv\n",
      "\n",
      "Data Quality Checks:\n",
      "Original CRSP records: 538814\n",
      "Original Compustat records: 599107\n",
      "Merged records: 528583\n",
      "Unique companies (permno): 6858\n",
      "Unique companies (gvkey): 7041\n",
      "Data time range: 2010-01-29 00:00:00 to 2021-12-31 00:00:00\n",
      "\n",
      "Summary of merged data (count, mean, min, max of key variables):\n",
      "       monthly_return          price    market_cap        revenue  \\\n",
      "count   525795.000000  528553.000000  5.285530e+05  466441.000000   \n",
      "mean         0.012373      35.835152  6.341127e+03    1055.243503   \n",
      "min         -0.993600    -205.015000  8.833500e-02   -9010.000000   \n",
      "max         19.883589  114600.000000  2.902368e+06  152079.000000   \n",
      "\n",
      "          net_income  \n",
      "count  527518.000000  \n",
      "mean       72.163762  \n",
      "min    -49746.000000  \n",
      "max     39646.000000  \n"
     ]
    }
   ],
   "source": [
    "# 5. Merging\n",
    "base_dir = \"/Users/dyuan868/Desktop/gpas/4993/stat-4993-sp25/Data\"\n",
    "crsp_file = f\"{base_dir}/2_processed_data/crsp_with_industry.csv\"\n",
    "comp_file = f\"{base_dir}/1_raw_data/compustat_data_2009_2021.csv\"\n",
    "ccm_file = f\"{base_dir}/1_raw_data/ccm_link_table.csv\"\n",
    "\n",
    "# Read data\n",
    "print(\"Reading CRSP, Compustat and CCM link table...\")\n",
    "crsp_data = pd.read_csv(crsp_file)\n",
    "comp_data = pd.read_csv(comp_file)\n",
    "ccm_data = pd.read_csv(ccm_file)\n",
    "\n",
    "# Ensure date columns are in datetime format\n",
    "crsp_data['date'] = pd.to_datetime(crsp_data['date'])\n",
    "comp_data['datadate'] = pd.to_datetime(comp_data['datadate'])\n",
    "ccm_data['linkdt'] = pd.to_datetime(ccm_data['linkdt'])\n",
    "ccm_data['linkenddt'] = pd.to_datetime(ccm_data['linkenddt'])\n",
    "\n",
    "print(\"Processing and merging data (optimized method)...\")\n",
    "\n",
    "# Prepare Compustat data\n",
    "comp_data['year'] = comp_data['datadate'].dt.year\n",
    "comp_data['month'] = comp_data['datadate'].dt.month\n",
    "comp_data['quarter'] = comp_data['datadate'].dt.quarter\n",
    "comp_data['yearmonth'] = comp_data['year'] * 100 + comp_data['month']\n",
    "\n",
    "# Create a fiscal quarter identifier\n",
    "comp_data['fiscal_yearquarter'] = comp_data['fiscal_year'] * 10 + comp_data['fiscal_quarter']\n",
    "\n",
    "# Fill NaT linkenddt with future date in CCM data\n",
    "max_date = pd.to_datetime('2025-12-31')\n",
    "ccm_data['linkenddt'] = ccm_data['linkenddt'].fillna(max_date)\n",
    "\n",
    "# Step 1: Merge CRSP with CCM link table\n",
    "print(\"Step 1: Joining CRSP with CCM link table...\")\n",
    "crsp_ccm = pd.merge(\n",
    "    crsp_data,\n",
    "    ccm_data[['gvkey', 'permno', 'linkdt', 'linkenddt']],\n",
    "    on='permno',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Filter valid links - ensure date is within link effective period\n",
    "crsp_ccm = crsp_ccm[\n",
    "    (crsp_ccm['date'] >= crsp_ccm['linkdt']) & \n",
    "    (crsp_ccm['date'] <= crsp_ccm['linkenddt'])\n",
    "]\n",
    "\n",
    "# Step 2: Prepare for quarter-based merging\n",
    "# Create yearmonth fields for joining\n",
    "crsp_ccm['year'] = crsp_ccm['date'].dt.year\n",
    "crsp_ccm['month'] = crsp_ccm['date'].dt.month\n",
    "crsp_ccm['yearmonth'] = crsp_ccm['year'] * 100 + crsp_ccm['month']\n",
    "\n",
    "# Create a quarter field\n",
    "crsp_ccm['quarter'] = crsp_ccm['date'].dt.quarter\n",
    "\n",
    "# Step 3: Optimize the merging process\n",
    "print(\"Step 3: Performing optimized merge...\")\n",
    "\n",
    "# Method 1: Using a more efficient approach with merge_asof\n",
    "# Sort both dataframes by date\n",
    "comp_data_sorted = comp_data.sort_values('datadate')\n",
    "crsp_ccm_sorted = crsp_ccm.sort_values('date')\n",
    "\n",
    "# Group by gvkey to process each company separately but efficiently\n",
    "merged_results = []\n",
    "\n",
    "# Get list of unique gvkeys\n",
    "unique_gvkeys = crsp_ccm['gvkey'].unique()\n",
    "total_gvkeys = len(unique_gvkeys)\n",
    "\n",
    "print(f\"Processing {total_gvkeys} unique companies...\")\n",
    "\n",
    "# Process in smaller batches to provide progress updates\n",
    "batch_size = max(1, total_gvkeys // 10)  # Show 10 progress updates\n",
    "for i, gvkey_batch in enumerate([unique_gvkeys[i:i+batch_size] for i in range(0, total_gvkeys, batch_size)]):\n",
    "    batch_results = []\n",
    "    \n",
    "    for gvkey in gvkey_batch:\n",
    "        # Get CRSP data for this gvkey\n",
    "        crsp_subset = crsp_ccm_sorted[crsp_ccm_sorted['gvkey'] == gvkey]\n",
    "        \n",
    "        if crsp_subset.empty:\n",
    "            continue\n",
    "            \n",
    "        # Get Compustat data for this gvkey\n",
    "        comp_subset = comp_data_sorted[comp_data_sorted['gvkey'] == gvkey]\n",
    "        \n",
    "        if comp_subset.empty:\n",
    "            continue\n",
    "            \n",
    "        # For each CRSP date, find the most recent Compustat date that's not after the CRSP date\n",
    "        merged = pd.merge_asof(\n",
    "            crsp_subset, \n",
    "            comp_subset, \n",
    "            left_on='date', \n",
    "            right_on='datadate',\n",
    "            by='gvkey',\n",
    "            direction='backward'\n",
    "        )\n",
    "        \n",
    "        # Only keep rows where we found a match\n",
    "        merged = merged.dropna(subset=['datadate'])\n",
    "        \n",
    "        # Add to batch results\n",
    "        batch_results.append(merged)\n",
    "    \n",
    "    # Combine batch results\n",
    "    if batch_results:\n",
    "        batch_df = pd.concat(batch_results, ignore_index=True)\n",
    "        merged_results.append(batch_df)\n",
    "        \n",
    "    # Show progress\n",
    "    progress = min(100, int((i+1) * batch_size * 100 / total_gvkeys))\n",
    "    print(f\"Progress: {progress}% ({min((i+1)*batch_size, total_gvkeys)}/{total_gvkeys} companies processed)\")\n",
    "\n",
    "# Combine all results\n",
    "if merged_results:\n",
    "    merged_df = pd.concat(merged_results, ignore_index=True)\n",
    "    print(f\"Merge completed successfully, {len(merged_df)} records found.\")\n",
    "else:\n",
    "    merged_df = pd.DataFrame()\n",
    "    print(\"No matching records found.\")\n",
    "\n",
    "# Remove unnecessary columns\n",
    "columns_to_drop = ['linkdt', 'linkenddt']\n",
    "merged_df = merged_df.drop(columns=columns_to_drop, errors='ignore')\n",
    "\n",
    "# Save merged data\n",
    "output_file = f\"{base_dir}/2_processed_data/crsp_compustat_merged_{datetime.now().strftime('%Y%m%d')}.csv\"\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "print(f\"Merged CRSP-Compustat data saved to {output_file}\")\n",
    "\n",
    "# Data quality checks\n",
    "print(\"\\nData Quality Checks:\")\n",
    "print(f\"Original CRSP records: {len(crsp_data)}\")\n",
    "print(f\"Original Compustat records: {len(comp_data)}\")\n",
    "print(f\"Merged records: {len(merged_df)}\")\n",
    "print(f\"Unique companies (permno): {merged_df['permno'].nunique()}\")\n",
    "print(f\"Unique companies (gvkey): {merged_df['gvkey'].nunique()}\")\n",
    "if not merged_df.empty:\n",
    "    print(f\"Data time range: {merged_df['date'].min()} to {merged_df['date'].max()}\")\n",
    "\n",
    "# Display only essential summary information to save time\n",
    "print(\"\\nSummary of merged data (count, mean, min, max of key variables):\")\n",
    "key_vars = ['monthly_return', 'price', 'market_cap', 'revenue', 'net_income']\n",
    "key_vars = [var for var in key_vars if var in merged_df.columns]\n",
    "if key_vars:\n",
    "    print(merged_df[key_vars].describe().loc[['count', 'mean', 'min', 'max']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725e7900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disconnect from WRDS\n",
    "db.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
